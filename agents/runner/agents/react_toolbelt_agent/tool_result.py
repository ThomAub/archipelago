"""
Tool result processing for handling large outputs.

Simple head_tail truncation - reliable and predictable.
"""

from litellm import token_counter
from loguru import logger

from runner.agents.models import LitellmAnyMessage, LitellmOutputMessage

# Defaults for head_tail truncation
# With ReSum context summarization, we can afford larger results
MAX_RESULT_TOKENS = 24000  # ~24k tokens before truncation
HEAD_CHARS = 20000  # Keep first 20k chars
TAIL_CHARS = 5000  # Keep last 5k chars

# Absolute maximum - if result exceeds this even after truncation, return error
# This prevents absurdly large results from ever being added to context
ABSOLUTE_MAX_CHARS = 100000


def estimate_tokens(model: str, content: str) -> int:
    """Estimate token count for content."""
    try:
        return token_counter(model=model, text=content)
    except Exception:
        return len(content) // 4


def truncate_tool_messages(
    messages: list[LitellmAnyMessage],
    model: str,
) -> None:
    """
    Truncate text content in tool messages, preserving everything else (images, etc).

    Mutates messages in place. Uses head_tail strategy for truncation.
    """
    for msg in messages:
        if not isinstance(msg, LitellmOutputMessage):
            continue

        content = msg.content
        if not isinstance(content, str):
            continue

        # Check for absurdly large results first
        if len(content) > ABSOLUTE_MAX_CHARS * 2:
            logger.bind(message_type="tool_result").error(
                f"Tool result is extremely large ({len(content):,} chars), truncating"
            )
            msg.content = (
                f"Error: Tool returned extremely large output ({len(content):,} characters). "
                f"This exceeds the maximum allowed size. "
                f"Try a more specific query or break down the request."
            )
            continue

        tokens = estimate_tokens(model, content)

        if tokens <= MAX_RESULT_TOKENS:
            continue

        logger.bind(message_type="tool_result").warning(
            f"Tool result is too large ({tokens} tokens > {MAX_RESULT_TOKENS}), truncating"
        )

        # Head-tail truncation
        if len(content) <= HEAD_CHARS + TAIL_CHARS:
            continue  # Content is fine as-is

        head = content[:HEAD_CHARS]
        tail = content[-TAIL_CHARS:]
        omitted = len(content) - HEAD_CHARS - TAIL_CHARS
        processed = (
            f"{head}\n\n"
            f"[... {omitted:,} characters omitted. "
            f"Use more specific queries to access full data. ...]\n\n"
            f"{tail}"
        )

        # Final safety check - ensure truncated result is within absolute max
        if len(processed) > ABSOLUTE_MAX_CHARS:
            logger.bind(message_type="tool_result").error(
                f"Truncated result still too large ({len(processed):,} chars)"
            )
            msg.content = (
                f"Error: Tool output too large even after truncation. "
                f"Original: {len(content):,} chars. Try a more specific query."
            )
        else:
            msg.content = processed
