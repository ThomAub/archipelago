import json
import os
import sys
import time
from pathlib import Path
from typing import Any

import httpx
import pytest

# Add archipelago agents to path if available (for runner imports)
SMOKE_TEST_DIR = Path(__file__).parent
MCP_REPO_DIR = SMOKE_TEST_DIR.parent
ARCHIPELAGO_AGENTS = MCP_REPO_DIR.parent / "archipelago" / "agents"
if ARCHIPELAGO_AGENTS.exists():
    sys.path.insert(0, str(ARCHIPELAGO_AGENTS))

from conftest import (  # noqa: E402
    INITIAL_DATA_DIR,
    configure_mcp_servers,
    populate_filesystem,
)

# Load smoke test configuration
SMOKE_CONFIG_PATH = SMOKE_TEST_DIR / "smoke_config.json"


def load_smoke_config() -> dict:
    """Load config from smoke_config.json."""
    if SMOKE_CONFIG_PATH.exists():
        with open(SMOKE_CONFIG_PATH) as f:
            return json.load(f)
    # Default config if file doesn't exist
    return {
        "agent": {
            "name": "Smoke Test Agent",
            "config_id": "react_toolbelt_agent",
            "timeout": 300,
            "max_steps": 50,
        },
        "task_prompt": "List all available tools and test them.",
        "expected_tools": [],
        "required_tools": [],
        "orchestrator": {"model": "gemini/gemini-2.5-flash", "temperature": 0.0},
    }


SMOKE_CONFIG = load_smoke_config()

ARCO_VALIDATE_URL = "https://api.studio.mercor.com/arco/validate"


def _extract_tool_name_from_call(tc: Any) -> str | None:
    """Extract tool name from a tool_call object (dict or Pydantic)."""
    fn = tc.get("function", {}) if isinstance(tc, dict) else getattr(tc, "function", {})
    name = fn.get("name") if isinstance(fn, dict) else getattr(fn, "name", None)
    return name.lower() if name else None


def get_tools_called(output: Any) -> set[str]:
    """Extract tool names from agent output messages."""
    tools: set[str] = set()
    for msg in output.messages:
        if isinstance(msg, dict):
            role = msg.get("role")
            tool_name = msg.get("name")
            tool_calls = msg.get("tool_calls") or []
        else:
            role = getattr(msg, "role", None)
            tool_name = getattr(msg, "name", None)
            tool_calls = getattr(msg, "tool_calls", None) or []

        if role == "tool" and tool_name:
            tools.add(tool_name.lower())
        elif role == "assistant":
            for tc in tool_calls:
                if name := _extract_tool_name_from_call(tc):
                    tools.add(name)

    return tools


class TestArcoValidation:
    """Test that arco.toml and mise.toml are correctly configured."""

    def test_arco_mise_validation(self) -> None:
        """Validate arco.toml + mise.toml configuration via arco API."""
        arco_path = MCP_REPO_DIR / "arco.toml"
        mise_path = MCP_REPO_DIR / "mise.toml"

        assert arco_path.exists(), f"arco.toml not found at {arco_path}"
        assert mise_path.exists(), f"mise.toml not found at {mise_path}"

        arco_content = arco_path.read_text()
        mise_content = mise_path.read_text()

        response = httpx.post(
            ARCO_VALIDATE_URL,
            json={"arco_toml": arco_content, "mise_toml": mise_content},
            timeout=30,
        )

        assert response.status_code == 200, f"Arco API returned {response.status_code}"

        result = response.json()
        if not result.get("valid"):
            errors = result.get("errors", [])
            pytest.fail(f"Arco validation failed: {errors}")

        print("Arco + mise.toml validation passed")


# Check if runner is available (installed from archipelago/agents at runtime)
_runner_available = False
run_agent = None
AgentConfig = None

try:
    # fmt: off
    # ruff: noqa: I001
    from runner.main import main as _run_agent  # type: ignore[import-not-found]
    from runner.models import AgentConfig as _AgentConfig  # type: ignore[import-not-found]
    # fmt: on

    run_agent = _run_agent
    AgentConfig = _AgentConfig
    _runner_available = True
except ImportError:
    pass

RUNNER_AVAILABLE: bool = _runner_available


@pytest.mark.skipif(not RUNNER_AVAILABLE, reason="runner module not available")
class TestMCPAgent:
    """Full agent test for MCP server."""

    @pytest.fixture
    def agent_config(self):
        """Agent config from smoke_config.json."""
        if AgentConfig is None:
            pytest.skip("AgentConfig not available")
        cfg = SMOKE_CONFIG.get("agent", {})
        return AgentConfig(
            agent_config_id=cfg.get("config_id", "react_toolbelt_agent"),
            agent_name=cfg.get("name", "Smoke Test Agent"),
            agent_config_values={
                "timeout": cfg.get("timeout", 300),
                "max_steps": cfg.get("max_steps", 50),
                "max_toolbelt_size": cfg.get("max_toolbelt_size", 80),
                "tool_call_timeout": cfg.get("tool_call_timeout", 30),
                "llm_response_timeout": cfg.get("llm_response_timeout", 60),
            },
        )

    @pytest.fixture
    def initial_messages(self):
        """Task prompt from config."""
        return [
            {
                "role": "user",
                "content": SMOKE_CONFIG.get(
                    "task_prompt", "List all available tools and test them."
                ),
            }
        ]

    @pytest.fixture
    def orchestrator_extra_args(self):
        """LLM args from config."""
        orch = SMOKE_CONFIG.get("orchestrator", {})
        return {
            "temperature": orch.get("temperature", 0.0),
        }

    @pytest.mark.asyncio
    async def test_agent_completes_task(
        self,
        base_url: str,
        mcp_config: dict[str, Any],
        agent_config: Any,
        initial_messages: list[dict[str, str]],
        orchestrator_extra_args: dict[str, Any],
    ) -> None:
        """Run agent and verify it completes the task using expected tools."""
        from runner.agents.models import AgentStatus  # type: ignore[import-not-found]

        if run_agent is None:
            pytest.skip("run_agent not available")

        # 1. Populate test data
        print("\n[1] Populating test data...")
        if INITIAL_DATA_DIR.exists():
            result = await populate_filesystem(base_url, INITIAL_DATA_DIR)
            print(f"    Added {result.get('objects_added', 0)} files")
        else:
            pytest.fail(
                f"Test data directory not found: {INITIAL_DATA_DIR}\n"
                "Create initial_data/ with sample files for testing."
            )

        # 2. Configure MCP server
        print("\n[2] Configuring MCP server...")
        await configure_mcp_servers(base_url, mcp_config, timeout=300)
        print("    OK - MCP server configured")

        # 3. Run the agent
        print("\n[3] Running agent...")
        orch_config = SMOKE_CONFIG.get("orchestrator", {})
        model = os.environ.get(
            "ORCHESTRATOR_MODEL", orch_config.get("model", "gemini/gemini-2.5-flash")
        )
        print(f"   Model: {model}")

        trajectory_id = f"smoke_test_{int(time.time())}"

        output = await run_agent(
            trajectory_id=trajectory_id,
            initial_messages=initial_messages,
            mcp_gateway_url=f"{base_url}/mcp/",
            mcp_gateway_auth_token=None,
            agent_config=agent_config,
            orchestrator_model=model,
            orchestrator_extra_args=orchestrator_extra_args,
        )

        # Save output for debugging
        results_dir = SMOKE_TEST_DIR / "results"
        results_dir.mkdir(exist_ok=True)
        output_json: str = output.model_dump_json(indent=2)
        safe_model_name = model.replace("/", "-") if model else "unknown"
        (results_dir / f"agent_output_{safe_model_name}.json").write_text(output_json)

        print("\n[4] Agent Results:")
        print(f"    Status: {output.status}")
        print(f"    Time: {output.time_elapsed:.2f}s")
        print(f"    Messages: {len(output.messages)}")

        # 4. Verify status is completed
        assert output.status == AgentStatus.COMPLETED, (
            f"Agent did not complete successfully. Status: {output.status}"
        )
        print("    OK - Agent completed")

        # 5. Verify agent used expected tools (from config)
        tools_called = get_tools_called(output)
        expected_tools = SMOKE_CONFIG.get("expected_tools", [])
        for tool in expected_tools:
            if tool.lower() in tools_called:
                print(f"    OK - Used tool: {tool}")
            else:
                print(f"    WARN - Tool not used: {tool}")

        # Required tools must be used (from config)
        required_tools = SMOKE_CONFIG.get("required_tools", [])
        for tool in required_tools:
            assert tool.lower() in tools_called, (
                f"Required tool '{tool}' was not called by agent"
            )

        print("\n" + "=" * 60)
        print("SMOKE TEST PASSED")
        print("=" * 60)


class TestMCPAgentFallback:
    """Fallback test when runner not available. Tool calls are app-specific."""

    @pytest.mark.asyncio
    async def test_tools_callable(
        self, base_url: str, mcp_config: dict[str, Any]
    ) -> None:
        """Call all 15 docs tools directly to verify they work."""
        from fastmcp import Client as FastMCPClient

        # Skip if agent runner is available (use full test instead)
        if RUNNER_AVAILABLE:
            pytest.skip("Agent runner available - use TestMCPAgent instead")

        # 1. Populate test data from initial_data/
        print("\n[1] Populating test data...")
        if INITIAL_DATA_DIR.exists():
            result = await populate_filesystem(base_url, INITIAL_DATA_DIR)
            files_added = result.get("objects_added", 0)
            print(f"    Added {files_added} files to /filesystem")
            assert files_added > 0, "No test files were populated!"
        else:
            pytest.fail(
                f"Test data directory not found: {INITIAL_DATA_DIR}\n"
                "Create initial_data/ with sample files for testing."
            )

        # 2. Configure MCP server
        print("\n[2] Configuring MCP server...")
        await configure_mcp_servers(base_url, mcp_config, timeout=300)
        print("    OK - MCP server configured")

        # 3. Call all 15 tools directly
        print("\n[3] Testing all 15 MCP tools...")
        gateway_config = {
            "mcpServers": {
                "gateway": {
                    "transport": "streamable-http",
                    "url": f"{base_url}/mcp/",
                }
            }
        }

        results = {}
        async with FastMCPClient(gateway_config) as client:
            # 1. Test get_document_overview
            print("\n    [1/15] Testing get_document_overview...")
            try:
                result = await client.call_tool(
                    "get_document_overview", {"file_path": "/documents/sample.docx"}
                )
                results["get_document_overview"] = {"success": True}
                assert len(str(result)) > 50, "Overview seems incomplete"
                print("    OK - get_document_overview")
            except AssertionError:
                raise
            except Exception as e:
                results["get_document_overview"] = {"success": False, "error": str(e)}
                print(f"    FAIL - get_document_overview: {e}")

            # 2. Test read_document_content
            print("\n    [2/15] Testing read_document_content...")
            try:
                result = await client.call_tool(
                    "read_document_content", {"file_path": "/documents/sample.docx"}
                )
                results["read_document_content"] = {"success": True}
                assert len(str(result)) > 50, "Content seems too short"
                print("    OK - read_document_content")
            except AssertionError:
                raise
            except Exception as e:
                results["read_document_content"] = {"success": False, "error": str(e)}
                print(f"    FAIL - read_document_content: {e}")

            # 3. Test create_document
            print("\n    [3/15] Testing create_document...")
            try:
                await client.call_tool(
                    "create_document",
                    {
                        "directory": "/",
                        "file_name": "test_smoke.docx",
                        "content": [
                            {"type": "heading", "text": "Smoke Test Doc", "level": 1},
                            {"type": "paragraph", "text": "First paragraph"},
                            {"type": "paragraph", "text": "Second paragraph"},
                            {"type": "paragraph", "text": "Third paragraph"},
                        ],
                    },
                )
                results["create_document"] = {"success": True}
                print("    OK - create_document")
            except Exception as e:
                results["create_document"] = {"success": False, "error": str(e)}
                print(f"    FAIL - create_document: {e}")

            # 4. Test add_content_text
            print("\n    [4/15] Testing add_content_text...")
            try:
                await client.call_tool(
                    "add_content_text",
                    {
                        "file_path": "/test_smoke.docx",
                        "identifier": "body.p.0",
                        "position": "after",
                        "text": "Added paragraph via smoke test",
                    },
                )
                results["add_content_text"] = {"success": True}
                print("    OK - add_content_text")
            except Exception as e:
                results["add_content_text"] = {"success": False, "error": str(e)}
                print(f"    FAIL - add_content_text: {e}")

            # 5. Test edit_content_text
            print("\n    [5/15] Testing edit_content_text...")
            try:
                await client.call_tool(
                    "edit_content_text",
                    {
                        "file_path": "/test_smoke.docx",
                        "identifier": "body.p.1",
                        "new_text": "Edited paragraph content",
                    },
                )
                results["edit_content_text"] = {"success": True}
                print("    OK - edit_content_text")
            except Exception as e:
                results["edit_content_text"] = {"success": False, "error": str(e)}
                print(f"    FAIL - edit_content_text: {e}")

            # 6. Test apply_formatting
            print("\n    [6/15] Testing apply_formatting...")
            try:
                await client.call_tool(
                    "apply_formatting",
                    {
                        "file_path": "/test_smoke.docx",
                        "identifier": "body.p.0",
                        "bold": True,
                        "font_size": 14,
                    },
                )
                results["apply_formatting"] = {"success": True}
                print("    OK - apply_formatting")
            except Exception as e:
                results["apply_formatting"] = {"success": False, "error": str(e)}
                print(f"    FAIL - apply_formatting: {e}")

            # 7. Test page_margins (read)
            print("\n    [7/15] Testing page_margins...")
            try:
                result = await client.call_tool(
                    "page_margins",
                    {"file_path": "/test_smoke.docx", "action": "read"},
                )
                results["page_margins"] = {"success": True}
                print("    OK - page_margins")
            except Exception as e:
                results["page_margins"] = {"success": False, "error": str(e)}
                print(f"    FAIL - page_margins: {e}")

            # 8. Test page_orientation (read)
            print("\n    [8/15] Testing page_orientation...")
            try:
                result = await client.call_tool(
                    "page_orientation",
                    {"file_path": "/test_smoke.docx", "action": "read"},
                )
                results["page_orientation"] = {"success": True}
                print("    OK - page_orientation")
            except Exception as e:
                results["page_orientation"] = {"success": False, "error": str(e)}
                print(f"    FAIL - page_orientation: {e}")

            # 9. Test header_footer (read)
            print("\n    [9/15] Testing header_footer...")
            try:
                result = await client.call_tool(
                    "header_footer",
                    {
                        "file_path": "/test_smoke.docx",
                        "action": "read",
                        "location": "header",
                        "position": "first",
                    },
                )
                results["header_footer"] = {"success": True}
                print("    OK - header_footer")
            except Exception as e:
                results["header_footer"] = {"success": False, "error": str(e)}
                print(f"    FAIL - header_footer: {e}")

            # 10. Test comments (read)
            print("\n    [10/15] Testing comments...")
            try:
                result = await client.call_tool(
                    "comments",
                    {"file_path": "/test_smoke.docx", "action": "read"},
                )
                results["comments"] = {"success": True}
                print("    OK - comments")
            except Exception as e:
                results["comments"] = {"success": False, "error": str(e)}
                print(f"    FAIL - comments: {e}")

            # 11. Test add_image
            print("\n    [11/15] Testing add_image...")
            try:
                await client.call_tool(
                    "add_image",
                    {
                        "file_path": "/test_smoke.docx",
                        "identifier": "body.p.0",
                        "position": "after",
                        "image_path": "/images/sample_image.jpg",
                        "width": 2.0,
                        "height": 2.0,
                    },
                )
                results["add_image"] = {"success": True}
                print("    OK - add_image")
            except Exception as e:
                results["add_image"] = {"success": False, "error": str(e)}
                print(f"    FAIL - add_image: {e}")

            # 12. Test read_image (from document with images)
            print("\n    [12/15] Testing read_image...")
            try:
                result = await client.call_tool(
                    "read_image",
                    {
                        "file_path": "/documents/doc_with_images.docx",
                        "annotation": "body.img.0",
                    },
                )
                results["read_image"] = {"success": True}
                print("    OK - read_image")
            except Exception as e:
                results["read_image"] = {"success": False, "error": str(e)}
                print(f"    FAIL - read_image: {e}")

            # 13. Test modify_image
            print("\n    [13/15] Testing modify_image...")
            try:
                result = await client.call_tool(
                    "modify_image",
                    {
                        "file_path": "/documents/doc_with_images.docx",
                        "identifier": "body",
                        "image_index": 0,
                        "operation": "rotate",
                        "rotation": 90,
                    },
                )
                results["modify_image"] = {"success": True}
                print("    OK - modify_image")
            except Exception as e:
                results["modify_image"] = {"success": False, "error": str(e)}
                print(f"    FAIL - modify_image: {e}")

            # 14. Test delete_content_text
            print("\n    [14/15] Testing delete_content_text...")
            try:
                await client.call_tool(
                    "delete_content_text",
                    {"file_path": "/test_smoke.docx", "identifier": "body.p.2"},
                )
                results["delete_content_text"] = {"success": True}
                print("    OK - delete_content_text")
            except Exception as e:
                results["delete_content_text"] = {"success": False, "error": str(e)}
                print(f"    FAIL - delete_content_text: {e}")

            # 15. Test delete_document (cleanup)
            print("\n    [15/15] Testing delete_document...")
            try:
                await client.call_tool(
                    "delete_document", {"file_path": "/test_smoke.docx"}
                )
                results["delete_document"] = {"success": True}
                print("    OK - delete_document")
            except Exception as e:
                results["delete_document"] = {"success": False, "error": str(e)}
                print(f"    FAIL - delete_document: {e}")

        # Summary
        print("\n" + "=" * 60)
        print("Results Summary (15 tools):")
        passed = sum(1 for r in results.values() if r.get("success"))
        total = len(results)
        print(f"    {passed}/{total} tools passed")

        failed_tools = [k for k, v in results.items() if not v.get("success")]
        if failed_tools:
            print(f"    Failed: {', '.join(failed_tools)}")
        print("=" * 60)

        # Verify core tools worked (these are critical)
        core_tools = [
            "get_document_overview",
            "read_document_content",
            "create_document",
            "add_content_text",
            "apply_formatting",
            "page_margins",
            "delete_document",
        ]
        for tool in core_tools:
            assert results.get(tool, {}).get("success"), f"{tool} tool failed"
