import json
import os
import sys
import time
from pathlib import Path
from typing import Any

import httpx
import pytest

# Add archipelago agents to path if available (for runner imports)
SMOKE_TEST_DIR = Path(__file__).parent
MCP_REPO_DIR = SMOKE_TEST_DIR.parent
ARCHIPELAGO_AGENTS = MCP_REPO_DIR.parent / "archipelago" / "agents"
if ARCHIPELAGO_AGENTS.exists():
    sys.path.insert(0, str(ARCHIPELAGO_AGENTS))

from conftest import (  # noqa: E402
    INITIAL_DATA_DIR,
    configure_mcp_servers,
    populate_filesystem,
)

# Load smoke test configuration
SMOKE_CONFIG_PATH = SMOKE_TEST_DIR / "smoke_config.json"


def load_smoke_config() -> dict:
    """Load config from smoke_config.json."""
    if SMOKE_CONFIG_PATH.exists():
        with open(SMOKE_CONFIG_PATH) as f:
            return json.load(f)
    # Default config if file doesn't exist
    return {
        "agent": {
            "name": "Smoke Test Agent",
            "config_id": "react_toolbelt_agent",
            "timeout": 300,
            "max_steps": 50,
        },
        "task_prompt": "List all available tools and test them.",
        "expected_tools": [],
        "required_tools": [],
        "orchestrator": {"model": "gemini/gemini-2.5-flash", "temperature": 0.0},
    }


SMOKE_CONFIG = load_smoke_config()

ARCO_VALIDATE_URL = "https://api.studio.mercor.com/arco/validate"


def _extract_tool_name_from_call(tc: Any) -> str | None:
    """Extract tool name from a tool_call object (dict or Pydantic)."""
    fn = tc.get("function", {}) if isinstance(tc, dict) else getattr(tc, "function", {})
    name = fn.get("name") if isinstance(fn, dict) else getattr(fn, "name", None)
    return name.lower() if name else None


def get_tools_called(output: Any) -> set[str]:
    """Extract tool names from agent output messages."""
    tools: set[str] = set()
    for msg in output.messages:
        if isinstance(msg, dict):
            role = msg.get("role")
            tool_name = msg.get("name")
            tool_calls = msg.get("tool_calls") or []
        else:
            role = getattr(msg, "role", None)
            tool_name = getattr(msg, "name", None)
            tool_calls = getattr(msg, "tool_calls", None) or []

        if role == "tool" and tool_name:
            tools.add(tool_name.lower())
        elif role == "assistant":
            for tc in tool_calls:
                if name := _extract_tool_name_from_call(tc):
                    tools.add(name)

    return tools


class TestArcoValidation:
    """Test that arco.toml and mise.toml are correctly configured."""

    def test_arco_mise_validation(self) -> None:
        """Validate arco.toml + mise.toml configuration via arco API."""
        arco_path = MCP_REPO_DIR / "arco.toml"
        mise_path = MCP_REPO_DIR / "mise.toml"

        assert arco_path.exists(), f"arco.toml not found at {arco_path}"
        assert mise_path.exists(), f"mise.toml not found at {mise_path}"

        arco_content = arco_path.read_text()
        mise_content = mise_path.read_text()

        response = httpx.post(
            ARCO_VALIDATE_URL,
            json={"arco_toml": arco_content, "mise_toml": mise_content},
            timeout=30,
        )

        assert response.status_code == 200, f"Arco API returned {response.status_code}"

        result = response.json()
        if not result.get("valid"):
            errors = result.get("errors", [])
            pytest.fail(f"Arco validation failed: {errors}")

        print("Arco + mise.toml validation passed")


# Check if runner is available (including all required imports)
_runner_available = False
run_agent = None
AgentConfig = None
AgentStatus = None

try:
    from runner.agents.models import AgentStatus as _AgentStatus
    from runner.main import main as _run_agent
    from runner.models import AgentConfig as _AgentConfig

    run_agent = _run_agent
    AgentConfig = _AgentConfig
    AgentStatus = _AgentStatus
    _runner_available = True
except ImportError:
    pass

RUNNER_AVAILABLE: bool = _runner_available


@pytest.mark.skipif(not RUNNER_AVAILABLE, reason="runner module not available")
class TestMCPAgent:
    """Full agent test for MCP server."""

    @pytest.fixture
    def agent_config(self):
        """Agent config from smoke_config.json."""
        cfg = SMOKE_CONFIG.get("agent", {})
        return AgentConfig(
            agent_config_id=cfg.get("config_id", "react_toolbelt_agent"),
            agent_name=cfg.get("name", "Smoke Test Agent"),
            agent_config_values={
                "timeout": cfg.get("timeout", 300),
                "max_steps": cfg.get("max_steps", 50),
                "max_toolbelt_size": cfg.get("max_toolbelt_size", 80),
                "tool_call_timeout": cfg.get("tool_call_timeout", 30),
                "llm_response_timeout": cfg.get("llm_response_timeout", 60),
            },
        )

    @pytest.fixture
    def initial_messages(self):
        """Task prompt from config."""
        return [
            {
                "role": "user",
                "content": SMOKE_CONFIG.get(
                    "task_prompt", "List all available tools and test them."
                ),
            }
        ]

    @pytest.fixture
    def orchestrator_extra_args(self):
        """LLM args from config."""
        orch = SMOKE_CONFIG.get("orchestrator", {})
        return {
            "temperature": orch.get("temperature", 0.0),
        }

    @pytest.mark.asyncio
    async def test_agent_completes_task(
        self,
        base_url: str,
        mcp_config: dict,
        agent_config,
        initial_messages: list,
        orchestrator_extra_args: dict,
    ) -> None:
        """Run agent and verify it completes the task using expected tools."""
        if AgentStatus is None:
            pytest.skip("AgentStatus not available")

        # 1. Populate test data
        print("\n[1] Populating test data...")
        if INITIAL_DATA_DIR.exists():
            result = await populate_filesystem(base_url, INITIAL_DATA_DIR)
            files_added = result.get("objects_added", 0)
            print(f"    Added {files_added} files")
            assert files_added > 0, (
                "No test files were populated - check initial_data directory"
            )
        else:
            pytest.fail(f"Test data directory not found: {INITIAL_DATA_DIR}")

        # 2. Configure MCP server
        print("\n[2] Configuring MCP server...")
        await configure_mcp_servers(base_url, mcp_config, timeout=300)
        print("    OK - MCP server configured")

        # 3. Run the agent
        print("\n[3] Running agent...")
        orch_config = SMOKE_CONFIG.get("orchestrator", {})
        model = os.environ.get(
            "ORCHESTRATOR_MODEL", orch_config.get("model", "gemini/gemini-2.5-flash")
        )
        print(f"   Model: {model}")

        trajectory_id = f"smoke_test_{int(time.time())}"

        output = await run_agent(
            trajectory_id=trajectory_id,
            initial_messages=initial_messages,
            mcp_gateway_url=f"{base_url}/mcp/",
            mcp_gateway_auth_token=None,
            agent_config=agent_config,
            orchestrator_model=model,
            orchestrator_extra_args=orchestrator_extra_args,
        )

        # Save output for debugging
        results_dir = SMOKE_TEST_DIR / "results"
        results_dir.mkdir(exist_ok=True)
        output_json = output.model_dump_json(indent=2)
        (results_dir / f"agent_output_{model.replace('/', '-')}.json").write_text(
            output_json
        )

        print("\n[4] Agent Results:")
        print(f"    Status: {output.status}")
        print(f"    Time: {output.time_elapsed:.2f}s")
        print(f"    Messages: {len(output.messages)}")

        # 4. Verify status is completed
        assert output.status == AgentStatus.COMPLETED, (
            f"Agent did not complete successfully. Status: {output.status}"
        )
        print("    OK - Agent completed")

        # 5. Verify agent used expected tools (from config)
        tools_called = get_tools_called(output)
        expected_tools = SMOKE_CONFIG.get("expected_tools", [])
        for tool in expected_tools:
            if tool.lower() in tools_called:
                print(f"    OK - Used tool: {tool}")
            else:
                print(f"    WARN - Tool not used: {tool}")

        # Required tools must be used (from config)
        required_tools = SMOKE_CONFIG.get("required_tools", [])
        for tool in required_tools:
            assert tool.lower() in tools_called, (
                f"Required tool '{tool}' was not called by agent"
            )

        print("\n" + "=" * 60)
        print("SMOKE TEST PASSED")
        print("=" * 60)


class TestMCPAgentFallback:
    """Fallback test when runner not available. Tool calls are app-specific."""

    @pytest.mark.asyncio
    async def test_tools_callable(self, base_url: str, mcp_config: dict) -> None:
        """Call tools directly to verify they work."""
        from fastmcp import Client as FastMCPClient

        # Skip if agent runner is available (use full test instead)
        try:
            from runner.main import main as run_agent  # noqa: F401

            pytest.skip("Agent runner available - use TestMCPAgent instead")
        except ImportError:
            pass

        # 1. Populate test data from initial_data/
        print("\n[1] Populating test data...")
        if INITIAL_DATA_DIR.exists():
            result = await populate_filesystem(base_url, INITIAL_DATA_DIR)
            files_added = result.get("objects_added", 0)
            print(f"    Added {files_added} files to /filesystem")
            assert files_added > 0, "No test files were populated!"
        else:
            pytest.fail(
                f"Test data directory not found: {INITIAL_DATA_DIR}\n"
                "Create initial_data/ with sample files for testing."
            )

        # 2. Configure MCP server
        print("\n[2] Configuring MCP server...")
        await configure_mcp_servers(base_url, mcp_config, timeout=300)
        print("    OK - MCP server configured")

        # 3. Call tools directly
        print("\n[3] Testing MCP tools...")
        gateway_config = {
            "mcpServers": {
                "gateway": {
                    "transport": "streamable-http",
                    "url": f"{base_url}/mcp/",
                }
            }
        }

        results = {}
        async with FastMCPClient(gateway_config) as client:
            # Test list_files at root
            print("\n    Testing list_files...")
            try:
                list_result = await client.call_tool("list_files", {"path": "/"})
                results["list_files"] = {"success": True}
                list_str = str(list_result).lower()
                assert "documents" in list_str, "documents/ not found in listing"
                print("    OK - list_files (found documents/)")
            except AssertionError:
                raise
            except Exception as e:
                results["list_files"] = {"success": False, "error": str(e)}
                print(f"    FAIL - list_files: {e}")

            # Test list_files in subdirectory
            print("\n    Testing list_files (subdirectory)...")
            try:
                _ = await client.call_tool("list_files", {"path": "/documents"})
                results["list_files_subdir"] = {"success": True}
                print("    OK - list_files (subdir)")
            except Exception as e:
                results["list_files_subdir"] = {"success": False, "error": str(e)}
                print(f"    FAIL - list_files (subdir): {e}")

            # Test read_text_file
            print("\n    Testing read_text_file...")
            try:
                read_result = await client.call_tool(
                    "read_text_file", {"file_path": "/documents/sample.txt"}
                )
                results["read_text_file"] = {"success": True}
                result_str = str(read_result)
                assert "smoke testing" in result_str.lower(), (
                    "Expected content not found in file"
                )
                print("    OK - read_text_file (content verified)")
            except AssertionError:
                raise
            except Exception as e:
                results["read_text_file"] = {"success": False, "error": str(e)}
                print(f"    FAIL - read_text_file: {e}")

            # Test get_file_metadata (if available)
            print("\n    Testing get_file_metadata...")
            try:
                _ = await client.call_tool(
                    "get_file_metadata", {"file_path": "/documents/sample.txt"}
                )
                results["get_file_metadata"] = {"success": True}
                print("    OK - get_file_metadata")
            except Exception as e:
                error_str = str(e).lower()
                if "unknown tool" in error_str:
                    results["get_file_metadata"] = {"success": True, "skipped": True}
                    print("    SKIP - get_file_metadata: Tool not available")
                else:
                    results["get_file_metadata"] = {"success": False, "error": str(e)}
                    print(f"    FAIL - get_file_metadata: {e}")

            # Test read_image_file (if available)
            print("\n    Testing read_image_file...")
            try:
                image_result = await client.call_tool(
                    "read_image_file", {"file_path": "/images/image.jpg"}
                )
                results["read_image_file"] = {"success": True}
                result_str = str(image_result)
                assert len(result_str) > 100, "Image data seems too small"
                print("    OK - read_image_file")
            except AssertionError:
                raise
            except Exception as e:
                error_str = str(e).lower()
                if "unknown tool" in error_str:
                    results["read_image_file"] = {"success": True, "skipped": True}
                    print("    SKIP - read_image_file: Tool not available")
                else:
                    results["read_image_file"] = {"success": False, "error": str(e)}
                    print(f"    FAIL - read_image_file: {e}")

        # Summary
        print("\n" + "=" * 50)
        print("Results Summary:")
        passed = sum(1 for r in results.values() if r.get("success"))
        total = len(results)
        print(f"    {passed}/{total} tools passed")
        print("=" * 50)

        # Verify core tools worked
        assert results.get("list_files", {}).get("success"), "list_files tool failed"
        assert results.get("read_text_file", {}).get("success"), (
            "read_text_file tool failed - test data may not be populated correctly"
        )
